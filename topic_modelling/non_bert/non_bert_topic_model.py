"""Put together preprocessing and NMF/LDA model for topic modelling"""
import pickle
import os
import sys
import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation, NMF
import pyLDAvis
import pyLDAvis.sklearn
sys.path.append("../..")

from utility import parse_config, seed_everything, custom_print
from preprocess_class import Dataset, create_datasets
from model_base_class import BaseModel

class TopicModel(BaseModel):
    """
    Non-bert based Model for topic modelling
    Always specify preprocessing steps before training / predicting the model
    Functions to train the model and generate output: train, display_topics, assign_topic_labels
    Functions to predict: predict
    """
    def __init__(self, train_dataset = None, test_dataset = None, feature_engineer_type = "bow",
                 custom_print_in = True):
        """
        Constructs all the necessary attributes for the TopicModel Object.
        For model training only, users should specify train_dataset.
        For model prediction only, users should specify test_dataset, pickled_model, 
        topic_label and pickled_vectorizer.
        For model training and prediction, users should specify train_dataset, test_dataset.
        feature_engineer and custom_print are optional attributes to be specified when necessary.

        Parameters
        ----------
        train_dataset: Dataset class
            Store dataset for model to be trained on. May be None if the class is only used to
            predict the test dataset using pickled vectorizer and model.
        test_dataset: Dataset class
            Store dataset for model to be predicted on. May be None if the class is only used to
            train a model without predicting on test data.
        feature_engineer_type: str
            Specify the feature engineering to be inputted to the training model.
            Valid inputs: "bow", "tfidf"
        custom_print: bool
            True if logs are to be shown and written out in a log file. False otherwise.
        
        Attributes
        ----------
        self.training_vectorizer: Fitted vectorizer
            Store the tfidf/bow vectorizer that was fitted to the train dataset.
            Initialised as None when train_dataset is None. Else, initialise with the file path of
            the pickled vectorizer.
        self.topic_label: list of str
            Store the trained topics manually generated by inspecting topic key words and sampled
            documents in each topic. Length of list = number of topics
            Initialised as empty string when train_dataset is None. Else, initialise with the file
            path of topic_key_words csv file to read in topic labels.
        self.model: Fitted transformer
            Store model to be trained on train_dataset and/or predicted on test_dataset. Initialised
            as None when trained_dataset is not None.
            Else, initialise this attribute with the file path of the pickled model.
        """
        self.train_dataset = train_dataset
        self.test_dataset = test_dataset
        self.model = None
        self.topic_label = []
        self.training_vectorizer = None
        self.feature_engineer_type = feature_engineer_type
        self.custom_print = custom_print_in

    def train(self, training_model, num_of_topics, train_output_path):
        """
        Mainly to generate the fitted vectorizer, trained model, topic key words and topic labels
        for each text in trained_dataset.
        Fitted vectorizer and trained model are stored in pickled file.
        Topic key words are stored in a csv file with the first column as the Topic_label and the
        subsequent columns as top key words / phrases for each topic. Each row represents one topic.
        Topic labels will have columns "Text" and "Topic_label".
        Sample training doc that are used for generating topic labels are also stored as csv file
        with columns "Text" and "Topic_label"

        Parameters
        ----------
        training_model: str
            Specify the type of model that will be used to train topic modelling.
            valid inputs: "NMF", "LDA"
        num_of_topics: int
            Specify the number of topics to be generated by the training model.
        train_output_path: str
            Specify the directory for models to be saved to.
        
        Return
        ------
        DataFrame with 3 columns: Text, Tokens, Topic no
        """
        if self.custom_print:
            custom_print("------ Training model --------\n", logger = logger)

        if training_model == "LDA":
            training = LatentDirichletAllocation(n_components= num_of_topics,
                                                 doc_topic_prior=0.5,
                                                 topic_word_prior=0.5, random_state = 4263)

        elif training_model == "NMF":
            training = NMF(n_components = num_of_topics, init = 'nndsvd',
                           random_state = 4263, solver = 'cd')

        else:
            raise NameError("invalid model input!")

        self.model = training.fit(self.train_dataset.feature_engineer[2])
        trained_model = self.model.transform(self.train_dataset.feature_engineer[2])

        self.training_vectorizer = self.train_dataset.feature_engineer[1]

        vectorizer_name = f"training_{self.feature_engineer_type}_vectorizer_{num_of_topics}.pk"
        self.dump_model(self.training_vectorizer, train_output_path, vectorizer_name)

        model_name = f"training_{training_model}_model_{num_of_topics}.pk"
        self.dump_model(self.model, train_output_path, model_name)

        doc_topic_labels = pd.DataFrame(trained_model).idxmax(axis = 1)
        num_of_doc_per_topic = doc_topic_labels.value_counts().sort_index()

        if self.custom_print:
            custom_print("------ Distribution for number of documents in each topic --------\n",
                         logger = logger)
            for index, topic in enumerate(num_of_doc_per_topic):
                custom_print(f"Topic {index}: {topic}",logger = logger)

            custom_print(f"------{num_of_doc_per_topic.sum()} documents trained------",
                         logger = logger)
            custom_print(f"------ End of training for {num_of_topics} topics --------",
                         logger = logger)

        labelled_train_topics = pd.DataFrame({"Text": self.train_dataset.data['Text'],
                                              "Tokens": self.train_dataset.preprocessed_text,
                                              "Topic no": doc_topic_labels})

        return training.components_, labelled_train_topics

    def predict(self, test_output_path, pickled_model, pickled_vectorizer, topic_labels):
        """
        Generate test labels based on the trained model. Test labels are stored as a csv
        file with 2 columns "Text" and "Topic_label".

        Parameters
        ----------
        test_output_path: str
            Specify the directory for generated topic labels for each text in test_dataset
            to be saved to.
        pickled_model: String
            File path to fitted NMF/LDA model on the train_dataset bow/tfidf vectorizer
        pickled_vectorizer: String
            File path of the fitted bow/tfidf vectorizer on the train_dataset
        topic_labels: list of str
            List of labels for corresponding topics

        Return
        ------
        labelled_test: pandas dataframe with columns Text and Topic_label
        """
        with open(pickled_model, 'rb') as p_model:
            self.model = pickle.load(p_model)

        with open(pickled_vectorizer, 'rb') as p_vectorizer:
            self.training_vectorizer = pickle.load(p_vectorizer)
        self.topic_label = topic_labels

        test_data_fitted = self.training_vectorizer.transform(
            self.test_dataset.preprocessed_text.apply(" ".join))
        testing_labels = self.model.transform(test_data_fitted)

        assigned_topic = pd.DataFrame(testing_labels).idxmax(axis = 1)

        if self.custom_print:
            custom_print("-------- Exporting labelled topics ----------", logger = logger)
        labelled_test = pd.DataFrame({"Text": self.test_dataset.data['Text'],
                                      "Topic_no": assigned_topic})
        indexed_topic_label = pd.DataFrame({"Topic_label":self.topic_label}).reset_index()

        labelled_test = labelled_test.merge(indexed_topic_label, how = "left",
                                            left_on = "Topic_no", right_on = "index")
        labelled_test = labelled_test.loc[:, ['Text', 'Topic_label']]
        labelled_test_output_path = os.path.join(test_output_path, "full_labelled_test_dataset.csv")
        labelled_test.to_csv(labelled_test_output_path, index = False)

        if self.custom_print:
            custom_print(f"------ End of predicting for {len(self.topic_label)} topics --------",
                         logger = logger)
        return labelled_test

    def modify_dataset_stop_words_list(self, replace_stop_words_list = None, include_words = None,
                                       exclude_words = None):
        """
        Modify stop words list of stored dataset(s)
        Suggested include_words: ['taste', 'flavor', 'amazon', 'price', 'minute', 'time', 'year']
        Suggested exclude_words: ["not", "no", "least", "less", "last", "serious", "too",
                                 "again", "against", "already", "always", "cannot", 
                                 "few", "must", "only", "though"]

        Parameters
        ----------
        replace_stop_words_list: list of str
            Specify own list of stop words instead of using the default stop words list.
        include_words: list of str
            Specify additional words to the original stop words list.
            Default are just some words that can be considered to be added into the stop words list.
        exclude_words: list of str
            Specify words that should not be included in the stop words list.
            Default are just some words that can be considered to be removed from stop words list.
        """
        if self.train_dataset is not None:
            self.train_dataset.modify_stop_words_list(replace_stop_words_list,
                                                      include_words, exclude_words)

        if self.test_dataset is not None:
            self.test_dataset.modify_stop_words_list(replace_stop_words_list,
                                                      include_words, exclude_words)

    def preprocess_dataset(self, root_word_option = 0, remove_stop_words = True, lower_case = True,
                            word_form = None):
        """
        Preprocess stored dataset

        Parameters
        ----------
        root_word_option: int
            Specify if original words, stemmed words or lemmatized words to be used
            for feature engineering.
            valid inputs: 0 (original word), 1 (stemmed words), 2 (lemmatized words)
        remove_stop_words: bool
            True if stop words should be removed before feature engineering.
            False if all words are kept for feature engineering.
        lower_case: bool
            True if all the words should be lowercased, False otherwise.
        word_form: list of str
            Specific parts of sentence to be included for topic model training and prediction.
            valid word forms: "verb", "noun", "adjective", "preposition", "adverb"
        """
        if self.train_dataset is not None:
            self.train_dataset.preprocessing_text(root_word_option, remove_stop_words,
                                                  lower_case, word_form)

        if self.test_dataset is not None:
            self.test_dataset.preprocessing_text(root_word_option, remove_stop_words,
                                                 lower_case, word_form)

    def generate_feature_engineer(self, lower_case = True, ngrams = (1,1),
                                  max_doc = 1, min_doc = 1):
        """
        Create feature engineering in stored dataset

        Parameters
        ----------
        lower_case: bool
            True if all the words should be lowercased, False otherwise.
        ngrams: tuple of int
            Specify if the user wants unigram, bigram, trigrams or even mixture.
            (1,1) means only unigram, (1,2) means unigram and bigram
        max_doc: int
            in the range [0,1): if token appear in more than max_doc % of documents,
                                the word is not considered in the bag of words.
            for any integer n >= 1: the token can only appear in at most n documents.
            If min_doc > 1, max_doc must be < 1.
            If preprocessing small test dataset, max_doc should be 1 to prevent filter 
            until no words left in pool of words.
        min_doc: int
            in the range [0,1): if token appear in less than min_doc % of documents, 
                                the word is not considered in the bag of words
            for any integer n >= 1: the token must appear in at least n documents
            If preprocessing small test dataset, min_doc should be 1 to prevent filter
            until no words left in pool of words.
        """
        if self.train_dataset is not None:
            if self.feature_engineer_type == "bow":
                self.train_dataset.create_bow(lower_case, ngrams, max_doc, min_doc)

            elif self.feature_engineer_type == "tfidf":
                self.train_dataset.create_tfidf(lower_case, ngrams, max_doc, min_doc)

        if self.test_dataset is not None:
            if self.feature_engineer_type == "bow":
                self.test_dataset.create_bow(lower_case, ngrams, max_doc, min_doc)

            elif self.feature_engineer_type == "tfidf":
                self.test_dataset.create_tfidf(lower_case, ngrams, max_doc, min_doc)

    def display_topics(self, training_model, trained_topics, num_top_words, train_output_path):
        """
        Output the top key words / phrases for text to be allocated to each topics
        If training_model == "LDA", a visualisation of the semantic distance between each topic
        will be stored in the specified directory.

        Parameters
        ----------
        training_model: str
            Indicate whether the NMF or LDA model is used to generate the topics.
            valid inputs: "LDA", "NMF"
        trained_topics: components_ attribute of the trained LDA or NMF model
        num_top_words: int
            Number of top topic key words to be displayed for topic labelling.
        train_output_path: str
            Path to store the LDA topic visualisation when applicable.

        Return
        ------
        topic_key_words: list of list 
            Each element in the outer list represents one topic and each element 
            in the nested list represents a topic key word.
        """
        topic_key_words = []

        feature_names = self.train_dataset.feature_engineer[0].get_feature_names_out()

        for topic in trained_topics:
            topic_key_words.append([feature_names[i]
                            for i in topic.argsort()[:-num_top_words - 1:-1]])

        if training_model == "LDA":
            topic_vis_path = os.path.join(train_output_path,
                                          f"topic_vis_{len(trained_topics)}.html")
            panel = pyLDAvis.sklearn.prepare(self.model,
                                             self.train_dataset.feature_engineer[2],
                                             self.train_dataset.feature_engineer[0], mds='tsne')
            pyLDAvis.save_html(panel, topic_vis_path)

        if self.custom_print:
            custom_print("------ Generating key words and sample documents for each topic ------\n",
                         logger = logger)

        return topic_key_words

    def assign_topic_labels(self, labelled_train_topics, topic_key_words, num_top_documents,
                            train_output_path):
        """
        Manually assign topic labels to each topic
        Export sample texts for labelling, with columns Text and Topic_label,
        fully labelled text, with columns Text and Topic_label
        and topic key words with rows as the topics, first column as the topic label
        and the subsequent columns as the topic key words.

        Parameters
        ----------
        labelled_train_topics: pandas dataframe
            Consist 3 columns: Text, tokens, Topic no
        topic_key_words: list of list
            Each list consists of top key words for its respective topic
        num_top_documents: int
            Number of top documents to help in assigning topic label
        """
        sample_doc_labels = pd.DataFrame()
        for index, key_words in enumerate(topic_key_words):
            curr_topic_key_words = ", ".join(key_words)

            topic_samples = labelled_train_topics.loc[
                labelled_train_topics['Topic no'] == index, :].sample(n = num_top_documents,
                                                                      random_state=4263)

            if self.custom_print:
                custom_print(f"\nTopic {index}", logger = logger)
                custom_print(curr_topic_key_words, logger = logger)

                for text in topic_samples['Tokens']:
                    custom_print("\n" + ", ".join(text), logger = logger)

            topic_samples = topic_samples.loc[:, ['Text', "Topic no"]]
            sample_doc_labels = pd.concat([sample_doc_labels, topic_samples])

            self.set_topic_labels(index)

        topic_labels = pd.DataFrame({"Topic_label": self.topic_label}).reset_index()
        topic_labels.rename(columns = {"index":"Topic no"}, inplace = True)
        labelled_train_topics = labelled_train_topics.merge(topic_labels,
                                                            how = "left",
                                                            on = "Topic no").loc[:,
                                                                                 ['Text',
                                                                                  "Topic_label"]]
        sample_doc_labels = sample_doc_labels.merge(topic_labels, how = "left",
                                                    on = "Topic no").loc[:, ['Text', "Topic_label"]]

        full_topic_path = os.path.join(train_output_path, "full_training_doc.csv")
        sampled_topic_path = os.path.join(train_output_path, "sample_training_doc.csv")

        labelled_train_topics.to_csv(full_topic_path, index = False)
        sample_doc_labels.to_csv(sampled_topic_path, index = False)

        topic_key_words = pd.DataFrame(topic_key_words, index = self.topic_label).reset_index()
        topic_key_words.rename(columns = {"index": 'Topic_label'}, inplace = True)
        topic_key_words_path = os.path.join(train_output_path, "topic_key_words.csv")
        topic_key_words.to_csv(topic_key_words_path, index = False)

    def dump_model(self, model, train_output_path, model_name):
        """
        Store the specified model in pickle file format in the specified directory

        Parameters
        ----------
        model: fitted estimator or vectorizer to be stored as pickle file
        train_output_path: str
            Directory for the pickled model and topic visualisation to be stored at.
        model_name: str
            Name of the pickle file
        """
        train_output_path = os.path.join(train_output_path, model_name)
        pickle.dump(model, open(train_output_path, 'wb'))

    def set_topic_labels(self, topic_no):
        """
        Take in manual input of Topic label and update the topic label attribute in the class

        Parameters
        ----------
        topic_no: int
            Topic number from the training model.
        """
        curr_topic_label = input(f"Label for Topic {topic_no}: ")

        while len(curr_topic_label) == 0:
            curr_topic_label = input("Please reenter label for topic: ")
        self.topic_label.append(curr_topic_label)

    def churn_eval_metrics(self, labelled_test, num_top_documents, test_output_path):
        """
        Manual inspection to check if the texts are labelled to the right topic.
        Filter a sample of texts from test_dataset to generate a csv file 
        (test_sample_labels) with manual inputs 1 or 0 
        to show if the texts are correctly labelled.
        Output csv of 3 columns: "Topic label", "Sample text", "Prediction" 
        (1 if right prediction, 0 if wrong prediction).

        Parameters
        ----------
        labelled_test: pandas dataframe
            List of text from test_dataset together with their corresponding topic labels
        num_top_documents: int
            Specify number of documents to be sampled from each topic to determine if the 
            documents are allocated to the right topic label.
        test_output_path: str
            Directory for the sampled topic labels and manual inspection to be stored at.
        """

        if self.custom_print:
            custom_print("-------Evaluating training sample accuracy-------", logger = logger)
        topic_accuracy = []
        sample_labelling = pd.DataFrame()
        for topic in self.topic_label:
            curr_topic_samples = labelled_test.loc[labelled_test['Topic_label']== topic,
                                                   "Text"].sample(n=num_top_documents,
                                                                  random_state=4263)
            if self.custom_print:
                custom_print("\n--------Allocate next topic------------", logger = logger)
                custom_print(f"Current topic: {topic}", logger = logger)

            correct_labels = []
            for sample in curr_topic_samples:

                if self.custom_print:
                    custom_print("\n" + sample, logger = logger)
                curr_label = input("Correct label (1 or 0): ")

                while not curr_label.isdigit() or int(curr_label) not in [0,1]:
                    curr_label = input("Please reenter correct label (1 or 0): ")

                curr_label = int(curr_label)
                correct_labels.append(curr_label)

            sample_topic_labels = pd.DataFrame({"Sample text": curr_topic_samples,
                                                "Prediction": correct_labels})
            sample_topic_labels.insert(0, 'Topic_label', topic)

            sample_labelling = pd.concat([sample_labelling, sample_topic_labels])

            accuracy = sum(correct_labels) / num_top_documents
            topic_accuracy.append(accuracy)
            if self.custom_print:
                custom_print(f"{topic} topic testing accuracy: {accuracy}", logger = logger)

        sample_test_output_path = os.path.join(test_output_path, "test_sample_labels.csv")
        sample_labelling.to_csv(sample_test_output_path, index = False)
        average_topic_accuracy = sum(topic_accuracy) / len(topic_accuracy)

        if self.custom_print:
            custom_print(f"-------- Average topic accuracy: {str(average_topic_accuracy)} --------",
                         logger = logger)

def test(test_dataset_in, feature_engineer_type_in, replace_stop_words_list_in, include_words_in,
         exclude_words_in, root_word_option_in, remove_stop_words_in, lower_case_in, word_form_in,
         ngrams_in, max_doc_in, min_doc_in, test_output_path_in, pickled_model_in,
         pickled_vectorizer_in, topic_label_in):
    testModel = TopicModel(test_dataset=test_dataset_in,
                        feature_engineer_type=feature_engineer_type_in)
    testModel.modify_dataset_stop_words_list(replace_stop_words_list_in, include_words_in,
                                                exclude_words_in)
    testModel.preprocess_dataset(root_word_option_in, remove_stop_words_in,
                                    lower_case_in, word_form_in)
    testModel.generate_feature_engineer(lower_case_in, ngrams_in, max_doc_in, min_doc_in)

    topic_label_in = pd.read_csv(topic_label_in).iloc[:,0].tolist()
    testModel.predict(test_output_path_in, pickled_model_in, pickled_vectorizer_in,
                        topic_label_in)

if __name__ == "__main__":
    curr_dir = os.getcwd()
    config_path = os.path.join(curr_dir, 'non_bert_topic_modelling_config.yml')
    config_file = parse_config(config_path)
    model_choice = config_file['model_choice']
    training_model_in = config_file['model'][model_choice]['type_of_model']
    feature_engineer_type_in = config_file['model'][model_choice]['feature_engineer']
    train_file = config_file['model'][model_choice]['data_folder']
    isTrainer = config_file['model'][model_choice]['train']
    isTester = config_file['model'][model_choice]['test']
    num_of_topics_in = config_file['model'][model_choice]['num_of_topics']
    num_top_words_in = config_file['model'][model_choice]['num_top_words']
    num_top_documents_in = config_file['model'][model_choice]['num_top_documents']
    topic_label_in = config_file['model'][model_choice]['topic_label']
    replace_stop_words_list_in = config_file['model'][model_choice]['replace_stop_words_list']
    root_word_option_in = config_file['model'][model_choice]['root_word_option']
    include_words_in = config_file['model'][model_choice]['include_words']
    exclude_words_in = config_file['model'][model_choice]['exclude_words']
    remove_stop_words_in = config_file['model'][model_choice]['remove_stop_words']
    lower_case_in = config_file['model'][model_choice]['lower_case']
    word_form_in = config_file['model'][model_choice]['word_form']
    ngrams_in = (config_file['model'][model_choice]['ngrams_start'],
              config_file['model'][model_choice]['ngrams_end'])
    max_doc_in = config_file['model'][model_choice]['max_doc']
    min_doc_in = config_file['model'][model_choice]['min_doc']
    home_folder = os.path.abspath(os.path.join(os.getcwd(),'../..'))
    train_output_path_in = config_file['model'][model_choice]['train_output_path']
    if train_output_path_in is not None:
        train_output_path_in = os.path.join(curr_dir, train_output_path_in)
        if not os.path.exists(train_output_path_in):
            os.makedirs(train_output_path_in)
    test_output_path_in = config_file['model'][model_choice]['test_output_path']
    if test_output_path_in is not None:
        test_output_path_in = os.path.join(curr_dir, test_output_path_in)
        if not os.path.exists(test_output_path_in):
            os.makedirs(test_output_path_in)
    logging_path = config_file['model'][model_choice]['log_path']
    if not os.path.exists("/".join(logging_path.split("/")[:-1])):
        os.makedirs("/".join(logging_path.split("/")[:-1]))
    logging_path = os.path.join(curr_dir,config_file['model'][model_choice]['log_path'])
    pickled_model_in = config_file['model'][model_choice]['pickled_model']
    if pickled_model_in is not None:
        pickled_model_in = os.path.join(curr_dir, pickled_model_in)
    pickled_vectorizer_in = config_file['model'][model_choice]['pickled_vectorizer']
    if pickled_vectorizer_in is not None:
        pickled_vectorizer_in = os.path.join(curr_dir, pickled_vectorizer_in)

    data_df = pd.read_csv(os.path.join(home_folder,train_file))
    logger = open(logging_path, 'w', encoding="utf8")
    seed_everything()
    custom_print('---------------------------------\n',logger = logger)

    if isTrainer:
        if isTester:
            train_dataset_in, test_dataset_in = create_datasets(data_df)
        else:
            train_dataset_in, test_dataset_in = Dataset(data_df), None
        custom_print("------Preprocessing text data--------", logger = logger)
        trainModel = TopicModel(train_dataset=train_dataset_in, test_dataset = test_dataset_in,
                                feature_engineer_type = feature_engineer_type_in)
        trainModel.modify_dataset_stop_words_list(replace_stop_words_list_in, include_words_in,
                                                 exclude_words_in)
        trainModel.preprocess_dataset(root_word_option_in, remove_stop_words_in,
                                      lower_case_in, word_form_in)
        trainModel.generate_feature_engineer(lower_case_in, ngrams_in, max_doc_in, min_doc_in)
        training_component, labelled_no_topics = trainModel.train(training_model_in,
                                                                     num_of_topics_in,
                                                                     train_output_path_in)
        topic_key_words_in = trainModel.display_topics(training_model_in, training_component,
                                                    num_top_words_in, train_output_path_in)
        trainModel.assign_topic_labels(labelled_no_topics, topic_key_words_in, num_top_documents_in,
                                       train_output_path_in)

        if test_dataset_in is not None:
            topic_label_in = topic_key_words_in.iloc[:, 0].tolist()
            test_labels = trainModel.predict(test_output_path_in, pickled_model_in,
                                             pickled_vectorizer_in, topic_label_in)
            trainModel.churn_eval_metrics(test_labels, num_top_documents_in, test_output_path_in)

    elif isTester:
        test_dataset_in = Dataset(data_df)
        custom_print("------Preprocessing text data--------", logger = logger)

        custom_print('Testing complete!',logger = logger)
    logger.close()
