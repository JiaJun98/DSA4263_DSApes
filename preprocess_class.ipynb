{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from re import sub\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet #trying different stopword list from packages nltk has a small list of stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer, TfidfVectorizer #sklearn have larger list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\limzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\limzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\limzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\limzi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This dataset class includes all the necessary preprocessing and feature extraction \n",
    "to be experimented for the different non-Bert models to predict sentiment analysis \n",
    "and topic modelling\n",
    "\"\"\"\n",
    "class Dataset:\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        All the attributes will be stored in the form of dataframe\n",
    "        \"\"\"\n",
    "        self.sentiments = dataset['Sentiment']\n",
    "        self.date = dataset['Time']\n",
    "        self.text = dataset['Text']\n",
    "        self.tokenized_words = None\n",
    "        self.tokenized_no_stop_words = None\n",
    "        self.tokenized_sentence = None\n",
    "        self.stem = None\n",
    "        self.lemmatize = None\n",
    "        self.stop_words_list = list(ENGLISH_STOP_WORDS).copy()\n",
    "        self.stop_words_list.remove(\"no\")\n",
    "        self.stop_words_list.remove(\"not\")\n",
    "        self.bag_of_words = None #overwrite if want to include bigrams or trigrams\n",
    "        self.tfidf = None\n",
    "        self.doc2vec = None\n",
    "        self.word2vec = None\n",
    "    \n",
    "    def word_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Split each text into individual word token.\n",
    "        Tokens such as numbers and punctuations are not considered as words, hence removed.\n",
    "        \"\"\"\n",
    "        self.tokenized_words = self.text.apply(lambda x: [word for word in word_tokenize(x) if word.isalpha()])\n",
    "\n",
    "    def sentence_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Split each text into individual sentence\n",
    "        \"\"\"\n",
    "        self.tokenized_sentence =  self.text.apply(sent_tokenize)\n",
    "\n",
    "    def stemming(self, remove_stop_words = True):\n",
    "        \"\"\"\n",
    "        One of the possible preprocessing steps to be used for feature extractions.\n",
    "        May specify whether to remove stop words before carrying out stemming\n",
    "        \"\"\"\n",
    "        if remove_stop_words:\n",
    "            if self.tokenized_no_stop_words == None:\n",
    "                self.remove_stop_words()\n",
    "            words_to_stem = self.tokenized_no_stop_words\n",
    "        \n",
    "        else:\n",
    "            words_to_stem = self.tokenized_words\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        self.stem = words_to_stem.apply(lambda x: [ps.stem(word) for word in x])\n",
    "    \n",
    "    def lemmatization(self, remove_stop_words = True):\n",
    "        \"\"\"\n",
    "        One of the possible preprocessing steps to be used for feature extractions.\n",
    "        May specify whether to remove stop words before carrying out stemming.\n",
    "        \"\"\"\n",
    "        if remove_stop_words:\n",
    "            if self.tokenized_no_stop_words == None:\n",
    "                self.remove_stop_words()\n",
    "            words_to_lemmatize = self.tokenized_no_stop_words\n",
    "        \n",
    "        else:\n",
    "            words_to_lemmatize = self.tokenized_words\n",
    "\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        self.lemmatize = words_to_lemmatize.apply(lambda x: [wordnet_lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    def remove_stop_words(self):\n",
    "        \"\"\"\n",
    "        Check through the lower casing of tokenized words to see if they exist in the list of stop words\n",
    "        \"\"\"\n",
    "        if self.tokenized_words == None:\n",
    "            self.word_tokenizer()\n",
    "        self.tokenized_no_stop_words = self.tokenized_words.apply([lambda x: [word for word in x if word.lower() not in self.stop_words_list]])\n",
    "\n",
    "    def word_bagging(self, root_words = None, stop_words = False, ngrams = (1,1), max_doc = 0.95, min_doc = 0.05):\n",
    "        \"\"\"\n",
    "        This is to create the bag of words based on the ngrams specified\n",
    "        root_words: select a method to preprocess the words, namely stem and lemmatize\n",
    "        stop_words: specify if stop words should be removed before using other preprocessing methods\n",
    "                    like lemmatizing and stemming\n",
    "        ngrams:     specify if the user wants unigram, bigram, trigrams or even mixture\n",
    "                    (1,1) means only unigram, (1,2) means unigram and bigram\n",
    "        min_doc:    usually in the range [0,1]. if word/phrase appear in less than min_doc % of documents, \n",
    "                    the word is not considered in the bag of words\n",
    "        max_doc:    usually in the range [0,1]. if word/phrase appear in more than max_doc % of documents,\n",
    "                    the word is not considered in the bag of words.\n",
    "        \"\"\"\n",
    "        if root_words == \"stem\":\n",
    "            self.stemming(remove_stop_words = stop_words)\n",
    "            final_text = self.stem\n",
    "        \n",
    "        elif root_words == \"lemmatize\":\n",
    "            self.lemmatization(remove_stop_words=stop_words)\n",
    "            final_text = self.lemmatize\n",
    "\n",
    "        elif root_words == None:\n",
    "            if stop_words:\n",
    "                if self.tokenized_no_stop_words == None:\n",
    "                    self.remove_stop_words()\n",
    "                final_text = self.tokenized_no_stop_words\n",
    "            else:\n",
    "                if self.tokenized_words == None:\n",
    "                    self.word_tokenizer\n",
    "                final_text = self.tokenized_words\n",
    "\n",
    "        vectorizer = CountVectorizer(lowercase=False) #upper case words throughout the feedback may mean the customer is angry, hence negative\n",
    "        self.bag_of_words = vectorizer.fit_transform(final_text, ngram_range = ngrams, min_df = min_doc, max_df = max_doc)\n",
    "\n",
    "    def create_tfidf(self, root_words = None, stop_words = False, ngrams = (1,1), max_doc = 0.95, min_doc = 0.05):\n",
    "        \"\"\"\n",
    "        Possible feature extraction to be included in modelling for Sentiment analysis and Topic modelling\n",
    "        \"\"\"\n",
    "        if root_words == \"stem\":\n",
    "            self.stemming(remove_stop_words = stop_words)\n",
    "            final_text = self.stem\n",
    "        \n",
    "        elif root_words == \"lemmatize\":\n",
    "            self.lemmatization(remove_stop_words=stop_words)\n",
    "            final_text = self.lemmatize\n",
    "\n",
    "        elif root_words == None:\n",
    "            if stop_words:\n",
    "                if self.tokenized_no_stop_words == None:\n",
    "                    self.remove_stop_words()\n",
    "                final_text = self.tokenized_no_stop_words\n",
    "            else:\n",
    "                if self.tokenized_words == None:\n",
    "                    self.word_tokenizer\n",
    "                final_text = self.tokenized_words\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(lowercase=False) #upper case words throughout the feedback may mean the customer is angry, hence negative\n",
    "        self.tfidf = vectorizer.fit_transform(final_text, ngram_range = ngrams, min_df = min_doc, max_df = max_doc)\n",
    "\n",
    "    def create_doc2vec(self, root_words = None, stop_words = False):\n",
    "        \"\"\"\n",
    "        Create vector representation of each text. Can use machine learning to see how to group\n",
    "        the texts under the same topic\n",
    "        root_words: select a method to preprocess the words, namely stem and lemmatize\n",
    "        stop_words: specify if stop words should be removed before using other preprocessing methods\n",
    "                    like lemmatizing and stemming\n",
    "\n",
    "        \"\"\"\n",
    "        if root_words == \"stem\":\n",
    "            self.stemming(remove_stop_words = stop_words)\n",
    "            final_text = self.stem\n",
    "        \n",
    "        elif root_words == \"lemmatize\":\n",
    "            self.lemmatization(remove_stop_words=stop_words)\n",
    "            final_text = self.lemmatize\n",
    "\n",
    "        elif root_words == None:\n",
    "            if stop_words:\n",
    "                if self.tokenized_no_stop_words == None:\n",
    "                    self.remove_stop_words()\n",
    "                final_text = self.tokenized_no_stop_words\n",
    "            else:\n",
    "                if self.tokenized_words == None:\n",
    "                    self.word_tokenizer()\n",
    "                final_text = self.tokenized_words\n",
    "                \n",
    "\n",
    "        tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(final_text)]\n",
    "        model = Doc2Vec(vector_size=100, window=5, workers=4, min_count=1, epochs=100)\n",
    "        model.build_vocab(tagged_docs)\n",
    "        model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "        # Get the document vectors\n",
    "        doc_vectors = []\n",
    "        for i in range(len(tagged_docs)):\n",
    "            doc_vector = model.infer_vector(tagged_docs[i].words)\n",
    "            doc_vectors.append(doc_vector)\n",
    "        self.doc2vec = doc_vectors        \n",
    "\n",
    "    def create_word2vec(self, root_words = None, stop_words = False):\n",
    "        \"\"\"\n",
    "        Create vector representation of each word. Each of these words can be taken in as input\n",
    "        for sentiment analysis and topic modelling\n",
    "        root_words: select a method to preprocess the words, namely stem and lemmatize\n",
    "        stop_words: specify if stop words should be removed before using other preprocessing methods\n",
    "                    like lemmatizing and stemming\n",
    "        \"\"\"\n",
    "        if root_words == \"stem\":\n",
    "            self.stemming(remove_stop_words = stop_words)\n",
    "            final_text = self.stem\n",
    "        \n",
    "        elif root_words == \"lemmatize\":\n",
    "            self.lemmatization(remove_stop_words=stop_words)\n",
    "            final_text = self.lemmatize\n",
    "\n",
    "        elif root_words == None:\n",
    "            if stop_words:\n",
    "                if self.tokenized_no_stop_words == None:\n",
    "                    self.remove_stop_words()\n",
    "                final_text = self.tokenized_no_stop_words\n",
    "            else:\n",
    "                if self.tokenized_words == None:\n",
    "                    self.word_tokenizer()\n",
    "                final_text = self.tokenized_words\n",
    "        model = Word2Vec(final_text, vector_size= 100, window=5, min_count=1, workers=4, sg=0, epochs=100)\n",
    "\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        preprocessed_corpus = vectorizer.fit_transform(final_text.apply(lambda x: \" \".join(x)))\n",
    "\n",
    "        word2vec_mapping = {}\n",
    "        for word in vectorizer.get_feature_names_out():\n",
    "            if word in model.wv.key_to_index:\n",
    "                word2vec_mapping[word] = model.wv.key_to_index[word]\n",
    "        \n",
    "        self.word2vec = word2vec_mapping\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 4263, stratify = df['Sentiment'])\n",
    "\n",
    "train_dataset = Dataset(train)\n",
    "test_dataset = Dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code LDA topic modelling code\n",
    "# import gensim\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# # Load your corpus of text\n",
    "# corpus = [\"This is the first sentence.\", \"This is the second sentence.\", \"And this is the third one.\", \"Is this the first sentence?\"]\n",
    "\n",
    "# # Train the Word2Vec model\n",
    "# tokenized_corpus = [gensim.utils.simple_preprocess(sentence) for sentence in corpus]\n",
    "# model = gensim.models.Word2Vec(tokenized_corpus, size=100, window=5, min_count=1, workers=4, sg=0, iter=100)\n",
    "\n",
    "# # Preprocess the text by removing stop words and punctuation\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "# preprocessed_corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# # Map each word to its corresponding Word2Vec embedding vector\n",
    "# word2vec_mapping = {}\n",
    "# for word in vectorizer.get_feature_names():\n",
    "#     if word in model.wv.vocab:\n",
    "#         word2vec_mapping[word] = model.wv[word]\n",
    "\n",
    "# # Initialize the LDA model with Word2Vec embeddings as input\n",
    "# lda_model = LdaModel(corpus=preprocessed_corpus, num_topics=3, id2word=word2vec_mapping, passes=10)\n",
    "\n",
    "# # Examine the resulting topic-word and document-topic distributions\n",
    "# topics = lda_model.show_topics()\n",
    "# for topic in topics:\n",
    "#     print(f\"Topic {topic[0]}: {topic[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7eb6d0ac8a911f2048ed045a4b8e3d0c84c023d3e3a8c702d4d1bcdcdce358b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
