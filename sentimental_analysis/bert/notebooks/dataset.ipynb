{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset,DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data,tokenizer_name,max_len):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=True)\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=max_len,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,     # Return attention mask\n",
    "            truncation=True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(tokenizer_name, batch_size,max_len, sentences,targets=None, predict_only=False):\n",
    "    \"\"\"Facilitate loading of data\n",
    "\n",
    "    @param tokenizer_name: Name of tokenizer, usually the name of the model being used\n",
    "    @max_len: Integer maximum length of sentence allowed\n",
    "    @batch_size: Integer batch size of samples loading into model\n",
    "    @shuffle: Boolean to decide whether to shuffle samples while loading into model\n",
    "    @sentences: List of data samples X\n",
    "    @targets: List of target variables, if any y\n",
    "    @predict_only: Boolean to check if the any targets should be used to load the dataset\n",
    "    @return: DataLoader object that generates data for input into model\n",
    "    \"\"\"\n",
    "    inputs, masks = preprocessing_for_bert(sentences,tokenizer_name,max_len)\n",
    "    if not predict_only:\n",
    "        labels = torch.tensor(targets)\n",
    "        data = TensorDataset(inputs, masks, labels)\n",
    "    else:\n",
    "        data = TensorDataset(inputs, masks)\n",
    "    sampler = RandomSampler(data) if not predict_only else SequentialSampler(data)\n",
    "    return DataLoader(data, sampler=sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_create_data_loader(tokenizer_name, batch_size,max_len, X_train,y_train, X_val,y_val):\n",
    "    \"\"\"Facilitate loading of full data; Overloaded function\n",
    "\n",
    "    \"\"\"\n",
    "    train_inputs, train_masks = preprocessing_for_bert(X_train,tokenizer_name,max_len)\n",
    "    train_labels = torch.tensor(y_train)\n",
    "    val_inputs, val_masks = preprocessing_for_bert(X_val,tokenizer_name,max_len)\n",
    "    val_labels = torch.tensor(y_val)\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "    full_train_sampler = RandomSampler(full_train_data)\n",
    "    return DataLoader(full_train_data, sampler=full_train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook dataset.ipynb to script\n",
      "[NbConvertApp] Writing 2647 bytes to dataset.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script dataset.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
