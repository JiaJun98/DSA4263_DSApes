{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import syspend\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from utility import parse_config, seed_everything, custom_print\n",
    "from preprocess_class import create_datasets\n",
    "from dataset import full_bert_data_loader,preprocessing_for_bert, create_data_loader, full_create_data_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DONE: Finish BERT class framework(with Trainer Arguments so can customise other BERT models(BERT small, medium large or OTHERS))\n",
    "##### DONE: Add Bert Dataset class (Inherit or create yourself)\n",
    "##### DONE: Run 1 iterations of BERT model and add the metrics measuring(This week lecture) in utility.py\n",
    "##### DONE: Add the config variables from yml\n",
    "##### PENDING: Remove some other data(like links)\n",
    "##### PENDING: predictions\n",
    "##### PENDING: See results or plot graph when adjusting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating BERTClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred= predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer arguments\n",
    "\n",
    "# Define the training loop\n",
    "def train(model_name, train_dataset, eval_dataset):\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= model_path, #Model predictions and checkpoints\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=0.01,\n",
    "        adam_epsilon = 1e-8, #Default\n",
    "        logging_dir=logging_path, #Tensorboard logs\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"epoch\", #No Default\n",
    "        logging_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\"\n",
    "    )\n",
    "\n",
    "    # Define the optimizer and scheduler\n",
    "    #optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    #num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "    #lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        #optimizers=optimizer,#Default AdamW\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    training = trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    curr_dir = os.getcwd()\n",
    "    config_path = os.path.join(curr_dir, 'bert_sentiment_config.yml')\n",
    "    config_file = parse_config(config_path)\n",
    "    model_name = config_file['model']['model_name']\n",
    "    n_classes = int(config_file['model']['n_classes'])\n",
    "    max_len = int(config_file['model']['max_len'])\n",
    "    batch_size = int(config_file['model']['batch_size'])\n",
    "    epochs = int(config_file['model']['epochs'])\n",
    "    learning_rate = float(config_file['model']['learning_rate'])\n",
    "    epsilon = float(config_file['model']['epsilon'])\n",
    "    train_file = config_file['model']['data_folder']\n",
    "    home_folder = os.path.abspath(os.path.join(os.getcwd(),'../..'))\n",
    "    model_path = os.path.join(curr_dir, config_file['model']['model_path'])\n",
    "    logging_path = os.path.join(curr_dir,config_file['model']['log_path'])\n",
    "        \n",
    "    data_df = pd.read_csv(os.path.join(home_folder,train_file))\n",
    "    logger = open(os.path.join(curr_dir, logging_path), 'w')\n",
    "    custom_print(f'Device availiable: {device}', logger = logger)\n",
    "    train_df, test_df = train_test_split(data_df, test_size = 0.2, random_state = 4263) #You are using slighly different for BERT\n",
    "    train_dataset, val_dataset = full_bert_data_loader(model_name,max_len, batch_size, True, train_df)\n",
    "    custom_print(\"Train_val dataset loaded\",logger = logger)\n",
    "    custom_print('Training model',logger = logger)\n",
    "    seed_everything()\n",
    "    custom_print('---------------------------------\\n',logger = logger)\n",
    "\n",
    "    custom_print(\"Hyperparameters:\",logger = logger)\n",
    "    custom_print(f\"model name: {model_name}\",logger = logger)\n",
    "    custom_print(f\"Number of epochs: {epochs}\",logger = logger)\n",
    "    custom_print(f\"number of classes: {n_classes}\",logger = logger)\n",
    "    custom_print(f\"max length: {max_len}\",logger = logger)\n",
    "    custom_print(f\"batch size: {batch_size}\",logger = logger)\n",
    "    custom_print(f\"learning rate: {learning_rate}\",logger = logger)\n",
    "    \n",
    "    trainer = train(model_name, train_dataset, val_dataset)\n",
    "    custom_print('Training complete!',logger = logger)\n",
    "    \n",
    "    custom_print('Showing Training and Evaluation metrics....',logger = logger)\n",
    "    #https://stackoverflow.com/questions/68806265/huggingface-trainer-logging-train-data\n",
    "    for obj in trainer.state.log_history:\n",
    "        for key,value in obj.items():\n",
    "            custom_print(f'{key}: {value}')\n",
    "    logger.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train.ipynb to script\n",
      "[NbConvertApp] Writing 5906 bytes to train.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script train.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
