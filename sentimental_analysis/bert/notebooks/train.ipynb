{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gjj980/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
<<<<<<< HEAD
=======
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import sys\n",
<<<<<<< HEAD
    "sys.path.append(\"../../..\")\n",
    "from utility import parse_config, seed_everything, custom_print\n",
    "from preprocess_class import create_dataset"
=======
    "sys.path.append(\"../..\")\n",
    "from utility import parse_config, seed_everything, custom_print\n",
    "from preprocess_class import create_datasets\n",
    "from dataset import full_bert_data_loader,preprocessing_for_bert, create_data_loader, full_create_data_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DONE: Finish BERT class framework(with Trainer Arguments so can customise other BERT models(BERT small, medium large or OTHERS))\n",
    "##### DONE: Add Bert Dataset class (Inherit or create yourself)\n",
    "##### DONE: Run 1 iterations of BERT model and add the metrics measuring(This week lecture) in utility.py\n",
    "##### DONE: Add the config variables from yml\n",
    "##### PENDING: Remove some other data(like links)\n",
    "##### PENDING: predictions\n",
    "##### PENDING: See results or plot graph when adjusting threshold"
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating BERTClassifier class"
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1110, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/transformers/trainer.py\", line 168, in <module>\n",
      "    import datasets\n",
      "ModuleNotFoundError: No module named 'datasets'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "  File \"/var/folders/j0/zzt4z9p97ns3p77mp_qltfq40000gn/T/ipykernel_57696/3257931397.py\", line 1, in <module>\n",
      "    from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup\n",
      "  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1100, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1112, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\n",
      "No module named 'datasets'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1003, in get_records\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/inspect.py\", line 979, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/inspect.py\", line 798, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred= predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
   "source": [
    "#Trainer arguments\n",
    "\n",
    "# Define the training loop\n",
    "def train(model_name, train_dataset, eval_dataset):\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
<<<<<<< HEAD
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=50,\n",
    "        save_total_limit=5,\n",
    "    )\n",
    "\n",
    "    # Define the optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=num_training_steps)\n",
=======
    "        output_dir= model_path, #Model predictions and checkpoints\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=0.01,\n",
    "        adam_epsilon = 1e-8, #Default\n",
    "        logging_dir=logging_path, #Tensorboard logs\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"epoch\", #No Default\n",
    "        logging_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\"\n",
    "    )\n",
    "\n",
    "    # Define the optimizer and scheduler\n",
    "    #optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    #num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "    #lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=num_training_steps)\n",
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
    "\n",
    "    # Define the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
<<<<<<< HEAD
    "        optimizer=optimizer,\n",
    "        scheduler=lr_scheduler,\n",
    "        compute_metrics=compute_metrics,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model"
=======
    "        #optimizers=optimizer,#Default AdamW\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    training = trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver class"
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating BERTClassifier class"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device availiable: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_val dataset loaded\n",
      "Training model\n",
      "---------------------------------\n",
      "\n",
      "Hyperparameters:\n",
      "model name: bert-base-uncased\n",
      "Number of epochs: 1\n",
      "number of classes: 2\n",
      "max length: 64\n",
      "batch size: 32\n",
      "learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/gjj980/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3701\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 116\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ba339924a24a67a68cc1c3554facb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 654\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0541, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ef6a1c56474acdb29aa167e7523fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gjj980/miniforge3/envs/voc_project/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /Users/gjj980/Desktop/School/ASUS_MOVE/Y4S2/DSA4263/Deliveries/DSA4263_DSApes/sentimental_analysis/bert/models/checkpoint-116\n",
      "Configuration saved in /Users/gjj980/Desktop/School/ASUS_MOVE/Y4S2/DSA4263/Deliveries/DSA4263_DSApes/sentimental_analysis/bert/models/checkpoint-116/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.631949782371521, 'eval_accuracy': 0.7400611620795107, 'eval_precision': 0.5476905236184758, 'eval_recall': 0.7400611620795107, 'eval_f1': 0.6295072099235206, 'eval_runtime': 20.7908, 'eval_samples_per_second': 31.456, 'eval_steps_per_second': 1.01, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /Users/gjj980/Desktop/School/ASUS_MOVE/Y4S2/DSA4263/Deliveries/DSA4263_DSApes/sentimental_analysis/bert/models/checkpoint-116/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /Users/gjj980/Desktop/School/ASUS_MOVE/Y4S2/DSA4263/Deliveries/DSA4263_DSApes/sentimental_analysis/bert/models/checkpoint-116 (score: 0.631949782371521).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 529.1562, 'train_samples_per_second': 6.994, 'train_steps_per_second': 0.219, 'train_loss': 1.0540921441439925, 'epoch': 1.0}\n",
      "Training complete!\n",
      "Showing Training and Evaluation metrics\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainOutput' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m custom_print(\u001b[39m'\u001b[39m\u001b[39mShowing Training and Evaluation metrics\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[39m#https://stackoverflow.com/questions/68806265/huggingface-trainer-logging-train-data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m trainer\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39mlog_history:\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m key,value \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     41\u001b[0m         custom_print(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainOutput' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    curr_dir = os.getcwd()\n",
    "    os.chdir(\"../..\")\n",
    "    config_path = os.path.join(curr_dir, 'bert_sentiment_config.yml')\n",
    "    config_file = parse_config(config_path)\n",
    "    model_name = config_file['model']['model_name']\n",
    "    n_classes = int(config_file['model']['n_classes'])\n",
    "    max_len = int(config_file['model']['max_len'])\n",
    "    batch_size = int(config_file['model']['batch_size'])\n",
    "    epochs = int(config_file['model']['epochs'])\n",
    "    learning_rate = float(config_file['model']['learning_rate'])\n",
    "    epsilon = float(config_file['model']['epsilon'])\n",
    "    train_file = config_file['model']['train_file']\n",
    "    data_folder = os.path.join(os.getcwd(), config_file['model']['data_path'])\n",
    "    model_path = os.path.join(curr_dir, config_file['model']['model_path'])\n",
    "    logging_path = os.path.join(curr_dir,config_file['model']['log_path'])\n",
    "        \n",
    "    data_df = pd.read_csv(os.path.join(data_folder,train_file))\n",
    "    logger = open(os.path.join(curr_dir, logging_path), 'w')\n",
    "    custom_print(f'Device availiable: {device}')\n",
    "    train_df, test_df = train_test_split(data_df, test_size = 0.2, random_state = 4263) #You are using slighly different for BERT\n",
    "    train_dataset, val_dataset = full_bert_data_loader(model_name,max_len, batch_size, True, train_df)\n",
    "    custom_print(\"Train_val dataset loaded\")\n",
    "    custom_print('Training model')\n",
    "    seed_everything()\n",
    "    custom_print('---------------------------------\\n')\n",
    "\n",
    "    custom_print(\"Hyperparameters:\")\n",
    "    custom_print(f\"model name: {model_name}\")\n",
    "    custom_print(f\"Number of epochs: {epochs}\")\n",
    "    custom_print(f\"number of classes: {n_classes}\")\n",
    "    custom_print(f\"max length: {max_len}\")\n",
    "    custom_print(f\"batch size: {batch_size}\")\n",
    "    custom_print(f\"learning rate: {learning_rate}\")\n",
    "    \n",
    "    trainer = train(model_name, train_dataset, val_dataset)\n",
    "    custom_print('Training complete!')\n",
    "    \n",
    "    custom_print('Showing Training and Evaluation metrics....')\n",
    "    #https://stackoverflow.com/questions/68806265/huggingface-trainer-logging-train-data\n",
    "    for obj in trainer.state.log_history:\n",
    "        for key,value in obj.items():\n",
    "            custom_print(f'{key}: {value}')\n",
    "    logger.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train.ipynb to script\n",
      "[NbConvertApp] Writing 5732 bytes to train.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script train.ipynb"
>>>>>>> 054fc082e5b4f198ac41086f2c25cb5f3f3d9060
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
